{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Intensity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"font-size:15px;\">Copyright (c) 2024 Sepideh Nikookar</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import scipy.stats as stats\n",
    "from sklearn import metrics\n",
    "import scipy.signal as signal\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIFEBELL_DEV_SENSORS_DIR = '/path/to/Raw/ACC/Data'\n",
    "\n",
    "# Changing the current working directory\n",
    "os.chdir(LIFEBELL_DEV_SENSORS_DIR)\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"The Current working directory now is: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the directory\n",
    "files = os.listdir(format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the data from each record\n",
    "acc_data = []\n",
    "acc_fs = 50\n",
    "\n",
    "# Filter for files starting with \"patient_\" and ending with \".dat\"\n",
    "acc_files = [file for file in files if file.startswith('RawACCData_') and file.endswith('.dat')]\n",
    "\n",
    "# Loop through the patient files\n",
    "for file in acc_files:\n",
    "\n",
    "    # Extract the patient id from the filename\n",
    "    record_name = file.split('.')[0]\n",
    "    id = record_name.split('_')[1]\n",
    "    \n",
    "    # Load the record using wfdb.rdsamp()\n",
    "    acc_signal, header = wfdb.rdsamp(record_name)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(acc_signal, columns=['subject_id', 'session_id', 'packet_id', 'time_index', 'x', 'y', 'z', 'label'])\n",
    "    df['peripheral_id'] = id  # Add the patient ID to the DataFrame\n",
    "\n",
    "    start_time = int(re.search(r'\\d+', header['comments'][1]).group()) \n",
    "\n",
    "    # Convert time difference to milliseconds\n",
    "    time_diff_ms = (1/acc_fs) * 1000  # 1 second = 1000 milliseconds\n",
    "\n",
    "    # Adjust custom_index by subtracting 1, then convert it to time\n",
    "    df['inferred_epoch_ms'] = start_time + (df['time_index'] - 1) * time_diff_ms\n",
    "\n",
    "    # Append the current DataFrame to the list\n",
    "    acc_data.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "raw_acc_data = pd.concat(acc_data, ignore_index=True)\n",
    "\n",
    "raw_acc_data['inferred_timestamp_utc'] = pd.to_datetime(raw_acc_data.inferred_epoch_ms, unit='ms')\n",
    "\n",
    "# Sort the data\n",
    "raw_acc_data = raw_acc_data.sort_values(by=['peripheral_id', 'inferred_epoch_ms'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "raw_acc_data = raw_acc_data[[\n",
    "    'peripheral_id',\n",
    "    'subject_id', \n",
    "    'session_id',\n",
    "    'packet_id',\n",
    "    'inferred_epoch_ms', \n",
    "    'inferred_timestamp_utc',\n",
    "    'x', \n",
    "    'y', \n",
    "    'z', \n",
    "    'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels and Extract the dictionary part from the string\n",
    "label_mapping_str = header['comments'][0].split(\"Label mapping: \")[1]\n",
    "\n",
    "# Convert the string to a dictionary using ast.literal_eval\n",
    "label_mapping = ast.literal_eval(label_mapping_str)\n",
    "\n",
    "# Reverse the dictionary to map numeric values back to string labels\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "# Map 'Label_value' to string labels using the reversed dictionary\n",
    "raw_acc_data['label'] = raw_acc_data['label'].map(reverse_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIFEBELL_DEV_HR_DIR = '/path/to/HR/Data'\n",
    "\n",
    "# Changing the current working directory\n",
    "os.chdir(LIFEBELL_DEV_HR_DIR)\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"The Current working directory now is: {0}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the data from each record\n",
    "hr_data = []\n",
    "hr_fs = 1 #Hz\n",
    "\n",
    "\n",
    "# Filter for files starting with \"patient_\" and ending with \".dat\"\n",
    "hr_files = [file for file in files if file.startswith('HeartRate_') and file.endswith('.dat')]\n",
    "\n",
    "# Loop through the patient files\n",
    "for file in hr_files:\n",
    "\n",
    "    # Extract the patient id from the filename\n",
    "    record_name = file.split('.')[0]\n",
    "    id = record_name.split('_')[1]\n",
    "    \n",
    "    # Load the record using wfdb.rdsamp()\n",
    "    hr_signal, header = wfdb.rdsamp(record_name)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(hr_signal, columns=['packet_id', 'time_index', 'heart_rate'])\n",
    "    df['peripheral_id'] = id  # Add the patient ID to the DataFrame\n",
    "\n",
    "    start_time = int(re.search(r'\\d+', header['comments'][0]).group()) \n",
    "\n",
    "    # Convert time difference to milliseconds\n",
    "    time_diff_ms = (1/hr_fs) * 1000  # 1 second = 1000 milliseconds\n",
    "\n",
    "    # Adjust custom_index by subtracting 1, then convert it to time\n",
    "    df['inferred_epoch_ms'] = start_time + (df['time_index'] - 1) * time_diff_ms\n",
    "\n",
    "    # Append the current DataFrame to the list\n",
    "    hr_data.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "vivalnk_hr = pd.concat(hr_data, ignore_index=True)\n",
    "\n",
    "vivalnk_hr['patch_timestamp_utc'] = pd.to_datetime(vivalnk_hr.inferred_epoch_ms, unit='ms')\n",
    "\n",
    "# Sort the data\n",
    "vivalnk_hr = vivalnk_hr.sort_values(by=['peripheral_id', 'inferred_epoch_ms'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "vivalnk_hr = vivalnk_hr[[\n",
    "    'peripheral_id',\n",
    "    'packet_id',\n",
    "    'inferred_epoch_ms', \n",
    "    'patch_timestamp_utc',\n",
    "    'heart_rate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_data = raw_acc_data.copy()\n",
    "\n",
    "# --- design butterworth bandpass filter and apply it\n",
    "# filter parameters\n",
    "sampling_rate=50\n",
    "nyq = 0.5 * sampling_rate\n",
    "cutoff_fs=(0.05,2.0)\n",
    "filter_order=4\n",
    "\n",
    "nyq_cutoff = [i/nyq for i in cutoff_fs]\n",
    "filter_type='bandpass'\n",
    "bwfilter = signal.butter(\n",
    "    filter_order,\n",
    "    nyq_cutoff,\n",
    "    btype=filter_type,\n",
    "    analog=False,\n",
    "    output=\"sos\"\n",
    ")\n",
    "\n",
    "# sort the data in preparation for filtering\n",
    "acc_data.sort_values(['peripheral_id', 'inferred_epoch_ms'], inplace=True)\n",
    "\n",
    "# apply the butterworth filter\n",
    "acc_data[['x_filt', 'y_filt', 'z_filt']] = np.concatenate(\n",
    "    acc_data.groupby('peripheral_id')\n",
    "    .apply(\n",
    "        lambda x: signal.sosfiltfilt(\n",
    "            bwfilter, x[['x', 'y', 'z']].values, axis=0, padlen=0\n",
    "        )\n",
    "    ).values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- calculate magnitude and rolling magnitude values\n",
    "# Take the absolute value of the filtered axes columns\n",
    "acc_data[['x_filt','y_filt','z_filt']] = np.abs(acc_data[['x_filt','y_filt','z_filt']])\n",
    "acc_data['magnitude'] = np.sqrt(acc_data.x_filt**2 + acc_data.y_filt**2 + acc_data.z_filt**2)\n",
    "acc_data['inferred_timestamp_utc']=pd.to_datetime(acc_data['inferred_timestamp_utc'])\n",
    "\n",
    "# Sort the DataFrame by 'inferred_timestamp_utc'\n",
    "acc_data = acc_data.sort_values(by='inferred_timestamp_utc')\n",
    "\n",
    "# Compute rolling 30s activity count medians for each sample\n",
    "rolling_magnitude_tmp = acc_data.groupby(['peripheral_id', 'packet_id', 'subject_id', 'session_id', 'label']).rolling(\n",
    "    f'5s', \n",
    "    on='inferred_timestamp_utc', \n",
    "    ).median().reset_index(level=0)\n",
    "    \n",
    "rolling_magnitude_tmp['rolling_magnitude'] = rolling_magnitude_tmp.magnitude\n",
    "\n",
    "# Merge the counts columns back with the original activity_data\n",
    "acc_data = pd.merge(acc_data, rolling_magnitude_tmp[['peripheral_id', 'inferred_timestamp_utc', 'rolling_magnitude']], on=['peripheral_id', 'inferred_timestamp_utc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert inferred_timestamp_utc to microseconds\n",
    "acc_data['inferred_timestamp_utc'] = acc_data['inferred_timestamp_utc'].astype('datetime64[us]')\n",
    "vivalnk_hr['patch_timestamp_utc'] = vivalnk_hr['patch_timestamp_utc'].astype('datetime64[us]')\n",
    "\n",
    "# Sort dataframes by timestamp columns\n",
    "vivalnk_hr.sort_values('patch_timestamp_utc', inplace=True)\n",
    "acc_data.sort_values('inferred_timestamp_utc', inplace=True)\n",
    "\n",
    "# Define columns to keep\n",
    "hr_data_cols = ['peripheral_id', 'packet_id', 'patch_timestamp_utc', 'heart_rate']\n",
    "acc_data_cols = ['peripheral_id', 'packet_id',     'subject_id', 'session_id',  'inferred_epoch_ms', 'x', 'y', 'z', 'label', 'inferred_timestamp_utc', 'magnitude', 'rolling_magnitude']\n",
    "\n",
    "# Perform merge_asof\n",
    "data = pd.merge_asof(\n",
    "    vivalnk_hr[hr_data_cols],\n",
    "    acc_data[acc_data_cols],\n",
    "    left_on='patch_timestamp_utc',\n",
    "    right_on='inferred_timestamp_utc',\n",
    "    by=['peripheral_id', 'packet_id'],\n",
    "    tolerance=pd.Timedelta('0.15s'),\n",
    "    direction='nearest'\n",
    ")\n",
    "\n",
    "# Filter data to where we have labels\n",
    "data = data[data.label.notna()]\n",
    "\n",
    "# Display or further process `data` as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the last 20 minutes of each time series, this makes classes more balanced\n",
    "# only use the last 20 minutes of each time series - this cuts out all of the extra sitting from the respiration protocols\n",
    "# Group the data by 'id'\n",
    "grouped = data.groupby('peripheral_id')\n",
    "\n",
    "# Create an empty dataframe to store the filtered data\n",
    "filtered_data = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "# Iterate over each group\n",
    "for name, group in grouped:\n",
    "    # Calculate the cutoff time (i.e. 20 minutes ago from the latest timestamp in the group)\n",
    "    cutoff_time = group['inferred_timestamp_utc'].max() - timedelta(minutes=20)\n",
    "    \n",
    "    # Filter the data to only include rows within the last 25 minutes\n",
    "    filtered_group = group[group['inferred_timestamp_utc'] >= cutoff_time]\n",
    "    \n",
    "    # Append the filtered data to the overall dataframe\n",
    "    filtered_data = pd.concat([filtered_data, filtered_group])\n",
    "\n",
    "filtered_data['inferred_timestamp_utc'] = pd.to_datetime(filtered_data.inferred_timestamp_utc)\n",
    "filtered_data['x'] = filtered_data.x.astype(float)\n",
    "filtered_data['y'] = filtered_data.y.astype(float)\n",
    "filtered_data['z'] = filtered_data.z.astype(float)\n",
    "filtered_data['heart_rate'] = filtered_data.heart_rate.astype(int)\n",
    "\n",
    "# filter out invalid heart rate measurements from firmware\n",
    "filtered_data = filtered_data[filtered_data.heart_rate > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset to default Matplotlib style\n",
    "plt.style.use('default')\n",
    "\n",
    "plot_data = filtered_data\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plot_data[~plot_data.label.isin(['walking', 'jogging'])].rolling_magnitude.hist(density=True, alpha=0.5, bins=np.arange(0,1,0.005), label='Inactive State', edgecolor='black')\n",
    "plot_data[plot_data.label.isin(['walking', 'jogging'])].rolling_magnitude.hist(density=True, alpha=0.5, bins=np.arange(0,1,0.005), label='Active State', edgecolor='black')\n",
    "\n",
    "\n",
    "plt.axvline(0.07, c='darkred', label='Proposed Activity Threshold - 0.07')\n",
    "plt.xlabel('Activity Count (Median over 5s Window)', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Time Windows (Count)', fontweight='bold', fontsize=14)\n",
    "plt.legend()\n",
    "plt.title('Activity Count Values Separated by Activity \\n(Walking, Jogging vs Others)', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Define x-ticks including 0.07\n",
    "x_ticks = np.arange(0, 1.1, 0.2)  # Example ticks from 0 to 1 with step of 0.1\n",
    "x_ticks = np.append(x_ticks, 0.07)  # Add 0.07\n",
    "\n",
    "# Ensure x_ticks is unique and sorted\n",
    "x_ticks = np.unique(x_ticks)\n",
    "x_ticks.sort()\n",
    "\n",
    "# Update x-ticks\n",
    "plt.xticks(x_ticks)\n",
    "\n",
    "# Limit x-axis to 0 and 1\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_prediction = plot_data.rolling_magnitude > 0.07\n",
    "active_label = plot_data.label.isin(['walking', 'jogging'])\n",
    "print('No Modifications: \\n')\n",
    "print('Accuracy:', round(metrics.accuracy_score(active_label, active_prediction), 2))\n",
    "print('Balanced Accuracy:', round(metrics.balanced_accuracy_score(active_label, active_prediction), 2))\n",
    "print('Precision:', round(metrics.precision_score(active_label, active_prediction), 2))\n",
    "print('Recall:', round(metrics.recall_score(active_label, active_prediction), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with cleaned data - no interference, or activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = filtered_data[~filtered_data.label.isin(['interference', 'sitting_activity', 'standing_activity', 'activity', 'transition'])]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plot_data[~plot_data.label.isin(['walking', 'jogging'])].rolling_magnitude.hist(\n",
    "    density=True, alpha=0.5, bins=np.arange(0, 1, 0.005), label='Inactive State', edgecolor='black')\n",
    "plot_data[plot_data.label.isin(['walking', 'jogging'])].rolling_magnitude.hist(\n",
    "    density=True, alpha=0.5, bins=np.arange(0, 1, 0.005), label='Active State', edgecolor='black')\n",
    "\n",
    "plt.axvline(0.07, c='darkred', label='Proposed Activity Threshold - 0.07')\n",
    "plt.xlabel('Activity Count (Median over 5s Window)', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Time Windows (Count)', fontweight='bold', fontsize=14)\n",
    "plt.legend()\n",
    "plt.title('Activity Count Values Separated by Activity \\n(Walking, Jogging vs Others)', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Define x-ticks including 0.07\n",
    "x_ticks = np.arange(0, 1.1, 0.2)  # Example ticks from 0 to 1 with step of 0.1\n",
    "x_ticks = np.append(x_ticks, 0.07)  # Add 0.07\n",
    "\n",
    "# Ensure x_ticks is unique and sorted\n",
    "x_ticks = np.unique(x_ticks)\n",
    "x_ticks.sort()\n",
    "\n",
    "# Update x-ticks\n",
    "plt.xticks(x_ticks)\n",
    "\n",
    "# Limit x-axis to 0 and 1\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "plt.savefig('Active_vs_Inactive.png', transparent=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_prediction_refined = plot_data.rolling_magnitude > 0.07\n",
    "active_label_refined = plot_data.label.isin(['walking', 'jogging'])\n",
    "print('Accuracy:', round(metrics.accuracy_score(active_label_refined, active_prediction_refined), 2))\n",
    "print('Balanced Accuracy:', round(metrics.balanced_accuracy_score(active_label_refined, active_prediction_refined), 2))\n",
    "print('Precision:', round(metrics.precision_score(active_label_refined, active_prediction_refined), 2))\n",
    "print('Recall:', round(metrics.recall_score(active_label_refined, active_prediction_refined), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for initial model\n",
    "fpr_initial, tpr_initial, _ = roc_curve(active_label, active_prediction)\n",
    "roc_auc_initial = auc(fpr_initial, tpr_initial)\n",
    "\n",
    "# Compute ROC curve and ROC area for refined model\n",
    "# Replace active_prediction_refined and active_label_refined with your refined model predictions\n",
    "fpr_refined, tpr_refined, _ = roc_curve(active_label_refined, active_prediction_refined)\n",
    "roc_auc_refined = auc(fpr_refined, tpr_refined)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_initial, tpr_initial, color='blue', lw=2, label=f'Initial Model (AUC = {roc_auc_initial:.2f})')\n",
    "plt.plot(fpr_refined, tpr_refined, color='green', lw=2, label=f'Refined Model (AUC = {roc_auc_refined:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('True Positive Rate (TPR)', fontweight='bold', fontsize=14)\n",
    "plt.title('ROC Curve', fontweight='bold', fontsize=16)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('ROC.png', transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(active_label_refined, active_prediction_refined)\n",
    "\n",
    "row_sums = cm.sum(axis=1, keepdims=True)\n",
    "normalized_cm = cm / row_sums.astype(float)\n",
    "\n",
    "cm = np.round(normalized_cm, decimals=2)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['Active', 'Inactive']\n",
    "\n",
    "# Convert confusion matrix values to percentages\n",
    "cm_percent = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(7, 6))\n",
    "cmap = plt.get_cmap('Blues')\n",
    "norm = Normalize(vmin=cm_percent.min(), vmax=cm_percent.max())\n",
    "ax = sns.heatmap(cm_percent, cmap=cmap, cbar=True,\n",
    "                 xticklabels=classes, yticklabels=classes, \n",
    "                 annot=False)  # Disable default annotations\n",
    "\n",
    "# Add custom annotations with percentage formatting\n",
    "for i in range(cm_percent.shape[0]):\n",
    "    for j in range(cm_percent.shape[1]):\n",
    "        value = cm_percent[i, j]\n",
    "        text_color = 'black' if value < 50 else 'white'  # Choose text color based on value\n",
    "        ax.text(j + 0.5, i + 0.5, f'{value:.1f}%', \n",
    "                ha='center', va='center', color=text_color, \n",
    "                fontsize=14)\n",
    "\n",
    "# Customize labels and title\n",
    "plt.xlabel('Predicted label', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual Label', fontsize=14, fontweight='bold')\n",
    "plt.title('Cumulative Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=classes, yticklabels=classes)\n",
    "# plt.xlabel('Predicted Labels', fontweight='bold', fontsize=14)\n",
    "# plt.ylabel('True Labels', fontweight='bold', fontsize=14)\n",
    "plt.savefig('cm(active_vs_inactive).png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject-Level Views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for peripheral_id in filtered_data.peripheral_id.unique():\n",
    "#     plot_data = filtered_data[~filtered_data.label.isin(['interference', 'sitting_activity', 'standing_activity', 'activity', 'transition'])]\n",
    "#     plot_data = filtered_data[filtered_data.peripheral_id == peripheral_id]\n",
    "#     fig, axes = plt.subplots(figsize=(10,3))\n",
    "#     plot_data[~plot_data.label.isin(['walking', 'jogging'])].rolling_magnitude.hist(density=True, alpha=0.5, bins=np.arange(0,1,0.005), label='Inactive State', edgecolor='black')\n",
    "#     plot_data[plot_data.label.isin(['walking', 'jogging'])].rolling_magnitude.hist(density=True, alpha=0.5, bins=np.arange(0,1,0.005), label='Active State', edgecolor='black')\n",
    "\n",
    "\n",
    "#     plt.axvline(0.07, c='darkred', label='Proposed Activity Threshold - 0.07')\n",
    "#     plt.xlabel('Activity Count  (Median over 5s window)', fontweight='bold', fontsize=14)\n",
    "#     plt.ylabel('Time Windows (count)', fontweight='bold', fontsize=14)\n",
    "#     plt.legend()\n",
    "#     plt.title(f'Activity Count Values Separated by Activity \\n(Walking, Jogging vs All Other Postures, activity, interference, and transitions removed)\\nID:{peripheral_id}')\n",
    "#     plt.xlim(0,0.7)\n",
    "#     plt.ylim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity-Level Views:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset to default Matplotlib style\n",
    "plt.style.use('default')\n",
    "\n",
    "for label in filtered_data.label.unique():\n",
    "    \n",
    "    plot_data = filtered_data[filtered_data.label == label]\n",
    "    fig, axes = plt.subplots(figsize=(10,3))\n",
    "    plot_data.magnitude.hist(density=True, alpha=0.5, bins=np.arange(0,1,0.005), edgecolor='black')\n",
    "\n",
    "\n",
    "    plt.axvline(0.07, c='darkred', label='Proposed Activity Threshold - 0.07')\n",
    "    plt.xlabel('Activity Count  (Median over 5s window)', fontweight='bold', fontsize=10)\n",
    "    plt.ylabel('Time Windows (count)', fontweight='bold', fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.title(f'Activity Count Values:{label}')\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitals View at a Subject-Level through Motion Protocol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for peripheral_id in filtered_data.peripheral_id.unique():\n",
    "    plot_data = filtered_data[(filtered_data.peripheral_id == peripheral_id) & (filtered_data.heart_rate > 0)].copy()\n",
    "    plot_data.loc[:, 'active'] = plot_data['rolling_magnitude'] > 0.07\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(figsize=(15,5))\n",
    "    sns.scatterplot(\n",
    "        x=plot_data.patch_timestamp_utc, \n",
    "        y=plot_data.heart_rate,\n",
    "        hue=plot_data.label,\n",
    "        s=20,\n",
    "        style=plot_data.active\n",
    "        )\n",
    "\n",
    "    plt.xlabel('Patch Time (UTC)', fontweight='bold', fontsize=10)\n",
    "    plt.ylabel('Heart Rate', fontweight='bold', fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.title(f'Heart Rate Over Motion Protocol \\n ID: {peripheral_id}')\n",
    "    plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Resting Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_time = filtered_data[filtered_data.label=='walking'].groupby('peripheral_id').patch_timestamp_utc.min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resting_data = []\n",
    "for group, data in filtered_data.groupby('peripheral_id'):\n",
    "    walk_time = data[data.label == 'walking'].patch_timestamp_utc.min()\n",
    "    pre_walk_data = data[data.patch_timestamp_utc < walk_time]\n",
    "    resting_data.append(pre_walk_data[pre_walk_data.label.isin(['lying', 'sitting'])])\n",
    "resting_data = pd.concat(resting_data)\n",
    "resting_data['heart_rate'] = resting_data.heart_rate.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resting_hr = resting_data.groupby('peripheral_id').heart_rate.median().reset_index()\n",
    "resting_std = resting_data.groupby('peripheral_id').heart_rate.std().reset_index()\n",
    "resting_qrt25 = resting_data.groupby('peripheral_id').heart_rate.quantile(0.25).reset_index()\n",
    "resting_qrt75 = resting_data.groupby('peripheral_id').heart_rate.quantile(0.75).reset_index()\n",
    "filtered_data = filtered_data.merge(resting_hr, on='peripheral_id', suffixes=('','_resting'))\n",
    "filtered_data = filtered_data.merge(resting_std, on='peripheral_id', suffixes=('','_std'))\n",
    "filtered_data = filtered_data.merge(resting_qrt25, on='peripheral_id', suffixes=('','_qrt25'))\n",
    "filtered_data = filtered_data.merge(resting_qrt75, on='peripheral_id', suffixes=('','_qrt75'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['heart_rate_diff'] = filtered_data.heart_rate - filtered_data.heart_rate_resting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of 3D magnitude vs. heart rate deviation\n",
    "# Reset to default Matplotlib style\n",
    "plt.style.use('default')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_data.rolling_magnitude, filtered_data.heart_rate_diff, s=6, alpha=0.6, color='royalblue')\n",
    "plt.xlabel('Activity Count (5s Rolling Median)', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('Deviation from Resting Heart Rate (bpm)', fontweight='bold', fontsize=14)\n",
    "# plt.title('Relationship between Activity Count and Heart Rate Deviation')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('activity_vs_heart_rate_deviation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficient, p_value = stats.pearsonr(filtered_data.rolling_magnitude, filtered_data.heart_rate_diff)\n",
    "\n",
    "print(f'Pearson correlation coefficient: {correlation_coefficient}')\n",
    "print(f'P-value: {p_value}')\n",
    "\n",
    "# Check significance\n",
    "alpha = 0.05  # significance level\n",
    "if p_value < alpha:\n",
    "    print('The correlation is statistically significant.')\n",
    "else:\n",
    "    print('The correlation is not statistically significant.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in filtered_data.label.unique():\n",
    "    plt.subplots(figsize=(10,3))\n",
    "    filtered_data[filtered_data.label == label].heart_rate_diff.hist(density=True, bins=range(-30,100,2))\n",
    "    plt.title(f\"Deviation From Resting Heart Rate For Activity: {label}\")\n",
    "    plt.xlabel('Deviation from Resting Heart Rate (in bpm)')\n",
    "    plt.ylabel('Density (adds to 1)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Safe_Natal",
   "language": "python",
   "name": "safe_natal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
